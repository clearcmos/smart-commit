#!/bin/bash

# Smart Commit Setup Script
# Sets up OLLAMA integration and installs smart-commit to system path

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Check if running as root
if [[ $EUID -eq 0 ]]; then
    echo -e "${RED}Error: This script should not be run as root.${NC}"
    echo "It will use sudo when necessary."
    exit 1
fi

# Detect OS
detect_os() {
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        OS="linux"
        SHELL_PROFILE="$HOME/.bashrc"
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        OS="macos"
        SHELL_PROFILE="$HOME/.zshrc"
    else
        echo -e "${RED}Error: Unsupported operating system: $OSTYPE${NC}"
        exit 1
    fi
    
    echo -e "${BLUE}Detected OS: $OS${NC}"
    echo -e "${BLUE}Shell profile: $SHELL_PROFILE${NC}"
}

# Function to check if environment variable exists in shell profile
check_env_var() {
    local var_name="$1"
    if grep -q "^export $var_name=" "$SHELL_PROFILE" 2>/dev/null; then
        return 0
    else
        return 1
    fi
}

# Function to get current environment variable value
get_env_var() {
    local var_name="$1"
    grep "^export $var_name=" "$SHELL_PROFILE" 2>/dev/null | cut -d'=' -f2- | tr -d '"'
}

# Function to check if smart-commit command is available in PATH
check_smart_commit_installed() {
    if command -v smart-commit >/dev/null 2>&1; then
        return 0
    else
        return 1
    fi
}

# Function to validate current environment configuration
validate_current_config() {
    local config_valid=true
    
    # Check for new variable names first, then fall back to old ones
    if check_env_var "AI_API_URL"; then
        local current_url=$(get_env_var "AI_API_URL")
        local current_model=$(get_env_var "AI_MODEL")
        local current_backend=$(get_env_var "AI_BACKEND_TYPE")
        
        if [ -z "$current_url" ]; then
            echo -e "${RED}✗ AI_API_URL is empty${NC}"
            config_valid=false
        fi
        if [ -z "$current_model" ]; then
            echo -e "${RED}✗ AI_MODEL is empty${NC}"
            config_valid=false
        fi
        if [ -z "$current_backend" ]; then
            echo -e "${YELLOW}⚠ AI_BACKEND_TYPE is empty (will auto-detect)${NC}"
        fi
    elif check_env_var "OLLAMA_API_URL"; then
        # Legacy configuration
        local current_url=$(get_env_var "OLLAMA_API_URL")
        local current_model=$(get_env_var "OLLAMA_MODEL")
        
        if [ -z "$current_url" ]; then
            echo -e "${RED}✗ OLLAMA_API_URL is empty${NC}"
            config_valid=false
        fi
        if [ -z "$current_model" ]; then
            echo -e "${RED}✗ OLLAMA_MODEL is empty${NC}"
            config_valid=false
        fi
    else
        echo -e "${RED}✗ No AI configuration found${NC}"
        config_valid=false
    fi
    
    if [ "$config_valid" = true ]; then
        return 0
    else
        return 1
    fi
}

# Function to check if Ollama is installed
check_ollama_installed() {
    if command -v ollama >/dev/null 2>&1; then
        return 0
    else
        return 1
    fi
}

# Function to check if Ollama service is running
check_ollama_running() {
    if check_ollama_installed; then
        # Check if ollama is responding
        if curl -s --max-time 5 http://localhost:11434/api/tags >/dev/null 2>&1; then
            return 0
        fi
    fi
    return 1
}

# Function to check if qwen3:8b model is available
check_qwen3_model() {
    if check_ollama_running; then
        if curl -s http://localhost:11434/api/tags | grep -q '"qwen3:8b"'; then
            return 0
        fi
    fi
    return 1
}

# Function to check if llama.cpp server is running locally
check_llamacpp_running() {
    # Check common ports for llama.cpp
    for port in 8080 8000 3000; do
        if curl -s --max-time 3 "http://localhost:$port/health" >/dev/null 2>&1; then
            echo "$port"
            return 0
        fi
    done
    return 1
}

# Function to get llama.cpp model information
get_llamacpp_model() {
    local port="$1"
    local model_info=$(curl -s --max-time 5 "http://localhost:$port/v1/models" 2>/dev/null)
    if [ -n "$model_info" ]; then
        echo "$model_info" | jq -r '.data[0].id' 2>/dev/null
    fi
}

# Function to probe existing llama.cpp installation (silent, returns config string)
probe_llamacpp_installation_silent() {
    local running_port
    if running_port=$(check_llamacpp_running); then
        local model_name=$(get_llamacpp_model "$running_port")
        if [ -z "$model_name" ] || [ "$model_name" = "null" ]; then
            model_name="auto-detected"
        fi
        # Return the configuration
        echo "http://localhost:$running_port|$model_name|llamacpp"
        return 0
    else
        return 1
    fi
}

# Function to probe existing llama.cpp installation (with output)
probe_llamacpp_installation() {
    echo -e "${BLUE}Probing for existing llama.cpp installation...${NC}"
    
    local running_port
    if running_port=$(check_llamacpp_running); then
        echo -e "${GREEN}✓ Found llama.cpp server running on port $running_port${NC}"
        
        local model_name=$(get_llamacpp_model "$running_port")
        if [ -n "$model_name" ] && [ "$model_name" != "null" ]; then
            echo -e "${GREEN}✓ Detected model: $model_name${NC}"
        else
            model_name="auto-detected"
            echo -e "${YELLOW}⚠ Could not detect specific model, will use auto-detection${NC}"
        fi
        
        return 0
    else
        echo -e "${RED}✗ No llama.cpp server found running on common ports (8080, 8000, 3000)${NC}"
        return 1
    fi
}

# Function to install Ollama
install_ollama() {
    echo -e "${BLUE}Installing Ollama...${NC}"
    
    if [[ "$OS" == "macos" ]]; then
        # Check if Homebrew is available
        if command -v brew >/dev/null 2>&1; then
            echo -e "${YELLOW}Installing Ollama via Homebrew...${NC}"
            brew install ollama
        else
            echo -e "${YELLOW}Installing Ollama via curl...${NC}"
            curl -fsSL https://ollama.com/install.sh | sh
        fi
    elif [[ "$OS" == "linux" ]]; then
        echo -e "${YELLOW}Installing Ollama via curl...${NC}"
        curl -fsSL https://ollama.com/install.sh | sh
    fi
    
    if check_ollama_installed; then
        echo -e "${GREEN}Ollama installed successfully!${NC}"
    else
        echo -e "${RED}Failed to install Ollama${NC}"
        exit 1
    fi
}

# Function to start Ollama service
start_ollama_service() {
    echo -e "${BLUE}Starting Ollama service...${NC}"
    
    if [[ "$OS" == "macos" ]]; then
        # On macOS, try to start Ollama in the background
        nohup ollama serve >/dev/null 2>&1 &
        sleep 3
    elif [[ "$OS" == "linux" ]]; then
        # On Linux, start Ollama service
        if command -v systemctl >/dev/null 2>&1; then
            sudo systemctl start ollama
            sudo systemctl enable ollama
        else
            nohup ollama serve >/dev/null 2>&1 &
            sleep 3
        fi
    fi
    
    # Wait for service to be ready
    local retries=10
    while [ $retries -gt 0 ]; do
        if check_ollama_running; then
            echo -e "${GREEN}Ollama service is running!${NC}"
            return 0
        fi
        sleep 2
        retries=$((retries - 1))
    done
    
    echo -e "${RED}Failed to start Ollama service${NC}"
    return 1
}

# Function to download qwen3:8b model
download_qwen3_model() {
    echo -e "${BLUE}Downloading qwen3:8b model (this may take several minutes)...${NC}"
    
    if ollama pull qwen3:8b; then
        echo -e "${GREEN}qwen3:8b model downloaded successfully!${NC}"
    else
        echo -e "${RED}Failed to download qwen3:8b model${NC}"
        exit 1
    fi
}

# Function to add or update environment variable in shell profile
add_env_var() {
    local var_name="$1"
    local var_value="$2"
    
    if check_env_var "$var_name"; then
        echo -e "${YELLOW}$var_name already exists in $SHELL_PROFILE${NC}"
        read -p "Do you want to update it? (y/N): " update_var
        if [[ $update_var =~ ^[Yy]$ ]]; then
            # Remove existing line and add new one
            # Use grep -v for cross-platform compatibility
            grep -v "^export $var_name=" "$SHELL_PROFILE" > "$SHELL_PROFILE.tmp" && mv "$SHELL_PROFILE.tmp" "$SHELL_PROFILE"
            echo "export $var_name=\"$var_value\"" >> "$SHELL_PROFILE"
            echo -e "${GREEN}Updated $var_name in $SHELL_PROFILE${NC}"
        else
            echo -e "${YELLOW}Keeping existing $var_name${NC}"
        fi
    else
        echo "export $var_name=\"$var_value\"" >> "$SHELL_PROFILE"
        echo -e "${GREEN}Added $var_name to $SHELL_PROFILE${NC}"
    fi
}

# Function to display current configuration status
show_current_config() {
    echo
    echo -e "${BLUE}Current Configuration Status${NC}"
    echo "============================"
    
    # Check smart-commit command availability
    if check_smart_commit_installed; then
        echo -e "${GREEN}✓${NC} smart-commit command: Available in PATH"
    else
        echo -e "${RED}✗${NC} smart-commit command: Not found in PATH"
    fi
    echo
    
    # Check for new variable names first, then fall back to old ones
    local api_url_var="AI_API_URL"
    local model_var="AI_MODEL"
    local backend_var="AI_BACKEND_TYPE"
    
    if check_env_var "$api_url_var"; then
        local current_url=$(get_env_var "$api_url_var")
        local current_model=$(get_env_var "$model_var")
        local current_backend=$(get_env_var "$backend_var")
    elif check_env_var "OLLAMA_API_URL"; then
        # Legacy configuration
        local current_url=$(get_env_var "OLLAMA_API_URL")
        local current_model=$(get_env_var "OLLAMA_MODEL")
        local current_backend="ollama"
        api_url_var="OLLAMA_API_URL"
        model_var="OLLAMA_MODEL"
    else
        current_url=""
    fi
    
    if [ -n "$current_url" ]; then
        echo -e "${GREEN}✓${NC} AI Configuration found:"
        echo -e "  API URL: ${YELLOW}$current_url${NC}"
        echo -e "  Model: ${YELLOW}$current_model${NC}"
        echo -e "  Backend: ${YELLOW}$current_backend${NC}"
        
        # Validate configuration
        echo
        echo -e "${BLUE}Configuration Validation:${NC}"
        if validate_current_config; then
            echo -e "${GREEN}✓ Configuration is valid${NC}"
        else
            echo -e "${RED}✗ Configuration has issues (see above)${NC}"
        fi
        
        if [[ "$current_url" == "http://localhost:11434" ]]; then
            echo -e "  Type: ${YELLOW}Local Ollama${NC}"
            if check_ollama_running; then
                echo -e "  Status: ${GREEN}✓ Running${NC}"
                if check_qwen3_model; then
                    echo -e "  Model Status: ${GREEN}✓ qwen3:8b available${NC}"
                else
                    echo -e "  Model Status: ${RED}✗ qwen3:8b not found${NC}"
                fi
            else
                echo -e "  Status: ${RED}✗ Not running${NC}"
            fi
        else
            if [[ "$current_backend" == "llamacpp" ]]; then
                echo -e "  Type: ${YELLOW}Remote Linux llama.cpp${NC}"
            else
                echo -e "  Type: ${YELLOW}Remote Windows Ollama${NC}"
            fi
        fi
    else
        echo -e "${RED}✗${NC} No AI configuration found"
    fi
    echo
}

# Function to setup local AI (Ollama for macOS, llama.cpp probe for Linux)
setup_local_ollama() {
    if [[ "$OS" == "macos" ]]; then
        setup_local_macos_ollama
    elif [[ "$OS" == "linux" ]]; then
        setup_local_linux_llamacpp
    fi
}

# Function to setup local Ollama on macOS
setup_local_macos_ollama() {
    echo -e "${BLUE}Setting up Local Ollama on macOS...${NC}"
    
    # Check if Ollama is already installed
    if check_ollama_installed; then
        echo -e "${GREEN}✓ Ollama is already installed${NC}"
    else
        install_ollama
    fi
    
    # Check if Ollama service is running
    if check_ollama_running; then
        echo -e "${GREEN}✓ Ollama service is already running${NC}"
    else
        start_ollama_service
    fi
    
    # Check if qwen3:8b model is available
    if check_qwen3_model; then
        echo -e "${GREEN}✓ qwen3:8b model is already available${NC}"
    else
        download_qwen3_model
    fi
    
    # Set up environment variables for local Ollama
    local api_url="http://localhost:11434"
    local model="qwen3:8b"
    local backend_type="ollama"
    
    echo -e "${BLUE}Setting up environment variables...${NC}"
    add_env_var "AI_API_URL" "$api_url"
    add_env_var "AI_MODEL" "$model"
    add_env_var "AI_BACKEND_TYPE" "$backend_type"
    
    # Set macOS local optimization flag
    add_env_var "SMART_COMMIT_MACOS_LOCAL" "true"
    echo -e "${YELLOW}Enabled macOS local optimization for better performance${NC}"
    
    echo -e "${GREEN}Local macOS Ollama setup complete!${NC}"
    echo -e "${YELLOW}API URL: $api_url${NC}"
    echo -e "${YELLOW}Model: $model${NC}"
    echo -e "${YELLOW}Backend: $backend_type${NC}"
}

# Function to setup local llama.cpp on Linux (probe existing installation)
setup_local_linux_llamacpp() {
    echo -e "${BLUE}Setting up Local AI on Linux...${NC}"
    echo
    echo -e "${YELLOW}⚠ WARNING: Linux local deployment is partially implemented${NC}"
    echo -e "${YELLOW}   This will detect and use your existing llama.cpp installation${NC}"
    echo -e "${YELLOW}   Full automated deployment is not yet available for Linux${NC}"
    echo
    
    # Probe for existing llama.cpp installation with output
    if probe_llamacpp_installation; then
        echo
        
        # Get the configuration silently
        local probe_result
        if probe_result=$(probe_llamacpp_installation_silent); then
            # Parse the result: "url|model|backend"
            local api_url=$(echo "$probe_result" | cut -d'|' -f1)
            local model=$(echo "$probe_result" | cut -d'|' -f2)
            local backend_type=$(echo "$probe_result" | cut -d'|' -f3)
            
            echo -e "${GREEN}✓ Successfully detected existing llama.cpp installation${NC}"
            echo -e "${BLUE}Setting up environment variables...${NC}"
            
            add_env_var "AI_API_URL" "$api_url"
            add_env_var "AI_MODEL" "$model"
            add_env_var "AI_BACKEND_TYPE" "$backend_type"
            
            echo -e "${GREEN}Local Linux llama.cpp setup complete!${NC}"
            echo -e "${YELLOW}API URL: $api_url${NC}"
            echo -e "${YELLOW}Model: $model${NC}"
            echo -e "${YELLOW}Backend: $backend_type${NC}"
        fi
    else
        echo
        echo -e "${RED}✗ No existing llama.cpp installation detected${NC}"
        echo -e "${YELLOW}Please choose a remote installation option instead.${NC}"
        echo
        echo "To set up llama.cpp locally, you would need to:"
        echo "1. Install llama.cpp from https://github.com/ggerganov/llama.cpp"
        echo "2. Download a compatible model (e.g., Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf)"
        echo "3. Start llama-server with appropriate parameters"
        echo
        exit 1
    fi
}

# Function to setup remote servers
setup_remote_server() {
    echo -e "${BLUE}Setting up Remote AI server...${NC}"
    echo
    echo "Choose your remote server type:"
    echo "1) Windows Ollama server (existing setup)"
    echo "2) Linux llama.cpp server (new setup)"
    echo
    
    read -p "Enter your choice (1-2): " server_choice
    
    case $server_choice in
        1)
            setup_remote_ollama
            ;;
        2)
            setup_remote_llamacpp
            ;;
        *)
            echo -e "${RED}Error: Invalid choice${NC}"
            exit 1
            ;;
    esac
}

# Function to setup remote Ollama (Windows)
setup_remote_ollama() {
    echo -e "${BLUE}Setting up Remote Windows Ollama server...${NC}"
    read -p "Enter the IP address of your Windows Ollama server (e.g., 192.168.1.2): " remote_ip
    
    # Validate IP address format (basic validation)
    if [[ ! $remote_ip =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]]; then
        echo -e "${RED}Error: Invalid IP address format${NC}"
        exit 1
    fi
    
    local api_url="http://$remote_ip:11434"
    local model="qwen3:8b"
    local backend_type="ollama"
    
    echo -e "${BLUE}Testing connection to remote Ollama server...${NC}"
    if curl -s --max-time 10 "$api_url/api/tags" >/dev/null 2>&1; then
        echo -e "${GREEN}✓ Successfully connected to remote Ollama server${NC}"
    else
        echo -e "${YELLOW}⚠ Warning: Could not connect to remote server (this is normal if the server is not running yet)${NC}"
    fi
    
    echo -e "${BLUE}Setting up environment variables...${NC}"
    add_env_var "AI_API_URL" "$api_url"
    add_env_var "AI_MODEL" "$model"
    add_env_var "AI_BACKEND_TYPE" "$backend_type"
    
    echo -e "${GREEN}Remote Windows Ollama setup complete!${NC}"
    echo -e "${YELLOW}API URL: $api_url${NC}"
    echo -e "${YELLOW}Model: $model${NC}"
    echo -e "${YELLOW}Backend: $backend_type${NC}"
}

# Function to setup remote llama.cpp (Linux)
setup_remote_llamacpp() {
    echo -e "${BLUE}Setting up Remote Linux llama.cpp server...${NC}"
    read -p "Enter the IP address of your Linux llama.cpp server (e.g., 192.168.1.3): " remote_ip
    
    # Validate IP address format (basic validation)
    if [[ ! $remote_ip =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]]; then
        echo -e "${RED}Error: Invalid IP address format${NC}"
        exit 1
    fi
    
    # Default port for llama.cpp server
    local default_port="8080"
    read -p "Enter the port (default: $default_port): " remote_port
    remote_port=${remote_port:-$default_port}
    
    local api_url="http://$remote_ip:$remote_port"
    local backend_type="llamacpp"
    
    echo -e "${BLUE}Testing connection to remote llama.cpp server...${NC}"
    if curl -s --max-time 10 "$api_url/health" >/dev/null 2>&1; then
        echo -e "${GREEN}✓ Successfully connected to remote llama.cpp server${NC}"
        
        # Try to get model name from the server
        local model_info=$(curl -s --max-time 5 "$api_url/v1/models" 2>/dev/null)
        if [ -n "$model_info" ]; then
            # Extract model name/path from response
            local model_name=$(echo "$model_info" | jq -r '.data[0].id' 2>/dev/null)
            if [ -n "$model_name" ] && [ "$model_name" != "null" ]; then
                echo -e "${GREEN}✓ Detected model: $model_name${NC}"
            else
                model_name="auto-detected"
            fi
        else
            model_name="auto-detected"
        fi
    else
        echo -e "${YELLOW}⚠ Warning: Could not connect to remote server (this is normal if the server is not running yet)${NC}"
        model_name="auto-detected"
    fi
    
    echo -e "${BLUE}Setting up environment variables...${NC}"
    add_env_var "AI_API_URL" "$api_url"
    add_env_var "AI_MODEL" "$model_name"
    add_env_var "AI_BACKEND_TYPE" "$backend_type"
    
    echo -e "${GREEN}Remote Linux llama.cpp setup complete!${NC}"
    echo -e "${YELLOW}API URL: $api_url${NC}"
    echo -e "${YELLOW}Model: $model_name${NC}"
    echo -e "${YELLOW}Backend: $backend_type${NC}"
}

# Function to setup OLLAMA integration
setup_ollama() {
    echo
    echo -e "${BLUE}OLLAMA Integration Setup${NC}"
    echo "========================"
    
    # Show current configuration status
    show_current_config
    
    echo "Choose your AI integration type:"
    if [[ "$OS" == "macos" ]]; then
        echo "1) Local Ollama (install and run locally)"
    elif [[ "$OS" == "linux" ]]; then
        echo "1) Local AI (probe existing llama.cpp installation)"
    fi
    echo "2) Remote AI server (Windows Ollama or Linux llama.cpp)"
    echo "3) Keep current configuration (if any)"
    echo
    
    read -p "Enter your choice (1-3): " ollama_choice
    
    case $ollama_choice in
        1)
            setup_local_ollama
            ;;
        2)
            setup_remote_server
            ;;
        3)
            # Check for any existing configuration
            if check_env_var "AI_API_URL" || check_env_var "OLLAMA_API_URL"; then
                echo -e "${GREEN}Keeping current configuration${NC}"
                
                if check_env_var "AI_API_URL"; then
                    local current_url=$(get_env_var "AI_API_URL")
                    local current_model=$(get_env_var "AI_MODEL")
                    local current_backend=$(get_env_var "AI_BACKEND_TYPE")
                    echo -e "${YELLOW}API URL: $current_url${NC}"
                    echo -e "${YELLOW}Model: $current_model${NC}"
                    echo -e "${YELLOW}Backend: $current_backend${NC}"
                else
                    # Legacy configuration
                    local current_url=$(get_env_var "OLLAMA_API_URL")
                    local current_model=$(get_env_var "OLLAMA_MODEL")
                    echo -e "${YELLOW}API URL: $current_url${NC}"
                    echo -e "${YELLOW}Model: $current_model${NC}"
                    echo -e "${YELLOW}Backend: ollama (legacy)${NC}"
                fi
                
                # Still need to validate and optionally install command
                echo
                echo -e "${BLUE}Configuration Validation:${NC}"
                if ! validate_current_config; then
                    echo -e "${RED}Current configuration has issues. Please fix or reconfigure.${NC}"
                    exit 1
                fi
            else
                echo -e "${RED}No current configuration found. Please choose option 1 or 2.${NC}"
                setup_ollama
            fi
            ;;
        *)
            echo -e "${RED}Error: Invalid choice${NC}"
            exit 1
            ;;
    esac
}

# Function to install smart-commit to system path
install_smart_commit() {
    echo
    echo -e "${BLUE}Installing smart-commit to system path...${NC}"
    
    # Check if smart-commit command is already available
    if check_smart_commit_installed; then
        echo -e "${GREEN}✓ smart-commit command is already available in PATH${NC}"
        
        # Check if we're in the smart-commit directory and offer to update
        if [[ -f "smart-commit.sh" ]]; then
            read -p "Do you want to update the installed smart-commit with the current version? (y/N): " update_cmd
            if [[ $update_cmd =~ ^[Yy]$ ]]; then
                echo -e "${YELLOW}Updating smart-commit in /usr/local/bin/ (requires sudo)...${NC}"
                sudo cp smart-commit.sh /usr/local/bin/smart-commit
                sudo chmod +x /usr/local/bin/smart-commit
                echo -e "${GREEN}smart-commit updated successfully!${NC}"
            else
                echo -e "${YELLOW}Keeping existing smart-commit installation${NC}"
            fi
        fi
        return 0
    fi
    
    # Check if smart-commit.sh exists
    if [[ ! -f "smart-commit.sh" ]]; then
        echo -e "${RED}Error: smart-commit.sh not found in current directory${NC}"
        echo -e "${YELLOW}Please run this setup script from the smart-commit directory${NC}"
        exit 1
    fi
    
    # Copy to /usr/local/bin with sudo
    echo -e "${YELLOW}Copying smart-commit to /usr/local/bin/ (requires sudo)...${NC}"
    sudo cp smart-commit.sh /usr/local/bin/smart-commit
    sudo chmod +x /usr/local/bin/smart-commit
    
    echo -e "${GREEN}smart-commit installed successfully!${NC}"
    echo -e "${YELLOW}You can now use 'smart-commit' from anywhere in your system.${NC}"
}

# Main setup function
main() {
    echo -e "${GREEN}Smart Commit Setup${NC}"
    echo "=================="
    echo
    
    # Detect OS and set shell profile
    detect_os
    
    # Create shell profile if it doesn't exist
    if [[ ! -f "$SHELL_PROFILE" ]]; then
        touch "$SHELL_PROFILE"
        echo -e "${YELLOW}Created $SHELL_PROFILE${NC}"
    fi
    
    # Setup OLLAMA integration
    setup_ollama
    
    # Install smart-commit to system path
    install_smart_commit
    
    echo
    echo -e "${GREEN}Setup completed successfully!${NC}"
    echo
    echo -e "${YELLOW}Important: Reload your shell or run 'source $SHELL_PROFILE' to use the new environment variables.${NC}"
    echo -e "${YELLOW}Then you can use 'smart-commit' from any git repository.${NC}"
    echo
    echo "Usage examples:"
    echo "  smart-commit              # Analyze changes and commit"
    echo "  smart-commit --dry-run    # Preview commit message without committing"
    echo "  smart-commit --full       # Generate detailed commit message without length limits"
    echo "  smart-commit --help       # Show help"
}

# Run main function
main "$@"